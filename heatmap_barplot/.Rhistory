##################################################
# phyloseq_2functionInk.R
##################################################
# In this script I aim to plot the abundances of the ESVs belonging to
# the different guilds identified with functionInk for each substrate.
#
# ZÃ¼rich, October 2020
# Theoretical Biology, ETH
# alberto.pascual.garcia@gmail.com
###################################################
rm(list=ls())
library(phyloseq)
library(reshape2)
#library(RDPutils) # Notused
library(ggplot2)
library(ggpubr) # for ggboxplot
library(BiotypeR)
library(this.path)
#extra=0 # Fixing this parameter to 1 will generate additional plots and print file outputs
# Edit options -----------------
# --- Select substrate that will be analysed
# substrate ="Agarose"
# stop.step= 266
min.module=4
substrates <- c("Alginate", "Agarose", "AgaroseAlginate", "AgaroseCarrageenan", "AgaroseChitosan", "Chitin", "Carrageenan")
for (substrate in substrates){
# minimum size of a functionInk module to be plotted
# --- Set working pathways
# dir.source="/home/ajf/Desktop/CNB/ecocoherence_sparcc"
# dir.funcInk=paste("/home/ajf/Desktop/CNB/ecocoherence_sparcc",substrate,sep="/")
# dirOut=paste(dir.funcInk,"figures",sep="/")
# dirOut
# dir.funcInk
# # --- Fix files
# fileOTU="/home/ajf/Desktop/CNB/marine_particles_source_data/count_table.ESV.4R.csv"
# fileSample="/home/ajf/Desktop/CNB/marine_particles_source_data/samples_properties/samples_metadata_AltSubstrSyntax.4R.tsv"
# fileFun=paste("/home/ajf/Desktop/CNB/ecocoherence_sparcc/",substrate,"/functionink_tmp/Partition-NL_Average_StopStep-",stop.step,"_interactions_filtered_p0.01_threshold_",substrate,"_.tsv",sep="")
work_dir<- this.dir()
dir.source=work_dir
home_dir <-  Sys.getenv("HOME")
dir.funcInk <- file.path(home_dir,"functionInk")
# dir.funcInk=paste("/home/ajf/Desktop/CNB/ecocoherence_sparcc",substrate,sep="/")
dir.funcInk <- file.path(work_dir,"..","functionink",substrate,"functionink_tmp")
dir.funcInk <- normalizePath(dir.funcInk)
# dirOut=paste(dir.funcInk,"figures",sep="/")
dirOut <- file.path(work_dir,"..","figures",substrate)
dirOut <- normalizePath(dirOut)
dir.funcInk
# --- Fix files
# fileOTU="/home/ajf/Desktop/CNB/marine_particles_source_data/count_table.ESV.4R.csv
fileOTU <- file.path(work_dir, "..", "data", "marine_particles_source_data", "count_table.ESV.4R.csv")
fileOTU <- normalizePath(fileOTU, mustWork = FALSE)
# fileSample="/home/ajf/Desktop/CNB/marine_particles_source_data/samples_properties/samples_metadata_AltSubstrSyntax.4R.tsv"
fileSample = file.path(work_dir, "..", "data", "marine_particles_source_data", "samples_properties","samples_metadata_AltSubstrSyntax.4R.tsv")
fileSample  <- normalizePath(fileSample, mustWork = FALSE)
# fileFun=paste("/home/ajf/Desktop/CNB/ecocoherence_sparcc/",substrate,"/functionink_tmp/Partition-NL_Average_StopStep-",stop.step,"_interactions_filtered_0.01_0.7_",substrate,"_.tsv",sep="")
# fileFun <- file.path(work_dir,"..","functionink",substrate,"functionink_tmp","functionink_tmp",
#                      paste("Partition-NL_Average_StopStep-",stop.step,"_interactions_filtered_0.01_threshold_",substrate,"_.tsv",sep=""))
# fileFun <- normalizePath(fileFun)
subs_path <- file.path(work_dir,"..","functionink",substrate)
subs_path <- normalizePath(subs_path)
setwd(subs_path)
# setwd(paste("/home/ajf/Desktop/CNB/ecocoherence_sparcc/", substrate, sep = ""))
dir_path <- "functionink_tmp/"
file <- list.files(
path = dir_path,
pattern = paste("Partition-NL_Average_StopStep-\\d+_interactions_filtered_p0.01_threshold_",substrate,"_.tsv$",sep=""),
full.names = F
)
# fileFun <- paste("Partition-NL_Average_StopStep-",stop.step,"_interactions_filtered_p0.01_threshold_",substrate,"_.tsv",sep="")
fileFun <- file
# paste("Partition-NL_Average_StopStep-",stop.step,"_interactions_filtered_p0.01_threshold_",substrate,"_.tsv",sep="") == "Partition-NL_Average_StopStep-608_interactions_filtered_p0.01_threshold_Agarose_.tsv"
######## STOP EDITING
# Load data ------------
setwd(dir.source)
# --- Load OTUs
otu.in=read.csv(fileOTU)
otu.pseq=otu_table(as.matrix(otu.in), taxa_are_rows = TRUE)
# --- Load samples metadata
sample_metadata = import_qiime_sample_data(fileSample)
# --- Load partition of functionInk
# ... we are going to use it as a taxonomy
setwd(dir.funcInk)
partition=read.table(fileFun,row.names = 1)
colnames(partition)="modules"
partition$modules=paste("mod",partition$modules,sep="_")
#Until here seems good------------------------------------------------------------
# Preprocessing (controls) ------------------
# --- Control that there are no empty rows/columns
sum(taxa_sums(otu.pseq) == 0) # 2046 have no observation
otu.pseq = prune_taxa(taxa_sums(otu.pseq) > 0, otu.pseq) # delete these guys
any(taxa_sums(otu.pseq) == 0) # double check (FALSE)
any(sample_sums(otu.pseq) == 0) # double check (FALSE)
ntaxa(otu.pseq)
# Process partition and create phyloseq object ----
# ... we create a vector with only the most relevant modules
partIds=unique(partition$modules)
partIds
part.count=vector(mode = "numeric",length = length(partIds))
part.count
names(part.count)=partIds
#This creates a vector with the counts of each module
for(part in partition$modules){
part.count[part]=part.count[part]+1
}
i=0
for(part in partition$modules){
i=i+1
if(part.count[part] < min.module){
partition$modules[i]="none"
}
}
length(partition$modules)
any(partition$modules == "none")
length(part.count)
sum(part.count > 4)
part.count
fileOut=paste(fileFun,"_guildGT",min.module,".txt",sep="")
fileOut
partition.out=as.data.frame(partition[which(partition$modules != "none"),])
rownames(partition.out)=rownames(partition)[which(partition$modules != "none")]
colnames(partition.out)="guild"
write.table(partition.out,file=fileOut,quote=FALSE,sep="\t")
partition.out
# ... then we map these modules to the ESVs
partition.all=matrix("none",nrow=dim(otu.in)[1],ncol=1)
rownames(partition.all)=rownames(otu.in)
matched=match(rownames(partition.all),rownames(partition))
partition.all[!is.na(matched),]=partition$modules[matched[!is.na(matched)]]
colnames(partition.all)="Modules"
partition.all
# ... let us say that the partition is the taxonomy
tax.pseq = tax_table(as.matrix(partition.all))
# ... Finally, create a single phyloseq object
particles=merge_phyloseq(otu.pseq,sample_metadata,tax.pseq)
idx=sort(as.numeric(levels(particles@sam_data$Time)),index.return=TRUE)$ix # reorder levels
particles@sam_data$Time=factor(particles@sam_data$Time,
levels(particles@sam_data$Time)[idx])
nsamples(particles)
particles@sam_data
# Process the whole dataset ---------
# ... Select a subset
particles.nosea = subset_samples(particles, Media != "Seawater") # exclude seawater from the analysis
particles.select = subset_samples(particles.nosea, Substrate == substrate) #"Alginate")
nsamples(particles.select)
# --- Before rarefying, we want to aggregate data within the same module
# .....  to create a new count table for LV estimation
# ..... The next steps are needed because there is a bug in the version of phyloseq I am using:
# ..... https://github.com/joey711/phyloseq/issues/223
fake.tax=as.matrix(particles.select@tax_table)
fake.tax=cbind(fake.tax,fake.tax)
fake.tax
colnames(fake.tax)=c("Modules","modules")
tax_table(particles.select) <- fake.tax
particles.select.aggr=tax_glom(particles.select, taxrank = "Modules") # agglomerate, by default returns one ESV belonging to that class
matched=match(rownames(particles.select.aggr@otu_table),
rownames(particles.select@tax_table))# and we want the classes in row names, so look for the class for each ESV
aggr.names=particles.select@tax_table[matched,1] # and use the classes as names for the otu table
aggr.names
rownames(particles.select.aggr@otu_table)=aggr.names
particles.select.aggr@otu_table
fileOut=paste("otu_table_",substrate,"_byModulesSize",min.module,".tsv",sep="")
write.table(particles.select.aggr@otu_table,file=fileOut,quote = FALSE, sep="\t")
#plot(particles.select.aggr@otu_table[3,],particles.select.aggr@otu_table[2,])
# --- Now yes, we work with a rarefied dataset
sampling=1000 # select the size here, 1018 is the minimum sampling sites, and the alpha diversity pattern is already there
set.seed(30052018) # Today's date 30/05/2018. Stored for reproducibility
particles.rar = rarefy_even_depth(particles.select, sample.size = sampling)
nsamples(particles.rar)
# ..... reorder the levels for time in particles.rar
particles.rar@sam_data$Time=as.factor(particles.rar@sam_data$Time)
idx=sort(as.numeric(levels(particles.rar@sam_data$Time)),index.return=TRUE)$ix # reorder levels
particles.rar@sam_data$Time=factor(particles.rar@sam_data$Time,
levels(particles.rar@sam_data$Time)[idx]) # reorder levels
#
# --- Create factors to separate replicates
#
sample_data(particles.rar)$RepTime <- as.factor(paste0(sample_data(particles.rar)$Time, sample_data(particles.rar)$Replica))
# #particles.rar.byRepTime =merge_samples(particles.rar, "RepTime") # don't merge
# ..... Now we want to sort this factor, but it is hard cause has numbers and letters, e.g. 204A
# ..... so we create a vector with the order we want, and then we match it
ii=sort(as.numeric(levels(particles.rar@sam_data$Time)),index.return=TRUE)$ix # Reorder time levels
times=as.character(levels(particles.rar@sam_data$Time)[ii]) # ordered times are our reference
order.fac=c()
for(i in 1:length(times)){
tmp.A=paste(times[i],"A",sep="") # Now create the strings per replica, e.g. 204A
tmp.B=paste(times[i],"B",sep="") # 204B
tmp.C=paste(times[i],"C",sep="") # 204C
tmp.vec=c(tmp.A,tmp.B,tmp.C)
order.fac=c(order.fac,tmp.vec) # And the final vector grows every cycle
}
# ..... And we can match now to this vector
matched=match(order.fac,levels(particles.rar@sam_data$RepTime))
particles.rar@sam_data$RepTime=factor(particles.rar@sam_data$RepTime,
levels(particles.rar@sam_data$RepTime)[matched]) # reorder level
particles.rar.byRepTime=particles.rar
# .... transform to proportions
particles.rar.freq= transform_sample_counts(particles.rar, function(x) 100 * x/sum(x))
particles.rar.byRepTime.freq = transform_sample_counts(particles.rar, function(x) 100 * x/sum(x))
# .... Perform some double checks
plot(colSums(particles.rar.byRepTime.freq@otu_table)) # all should be 100
length(which(particles.rar.byRepTime.freq@tax_table == "none")) #
dim(particles.rar.byRepTime.freq@otu_table) # 13120 ESVs
max(part.count) # the module with more ESVs has 66
length(which(particles.rar.byRepTime.freq@otu_table[,1] == 0)) # how many are zero in a random sample
idx.check=head(which(particles.rar.byRepTime.freq@otu_table[,1] == 0))
particles.rar.byRepTime.freq@otu_table[idx.check,1] # check that is indeed the case, are these represented in the bar plot at all?
# Associate preferences to the modules -----
# --- Extract the table for ESVs in modules
ESVs.modules.ids=which(particles.rar@tax_table != "none")
ESVs.modules=rownames(particles.rar@otu_table)[ESVs.modules.ids]
particles.rar.mod=prune_taxa(ESVs.modules, particles.rar)
ntaxa(particles.rar.mod)
nsamples(particles.rar.mod)
# .... aggregate taxa within the same module
# The next steps are needed because there is a bug in the version of phyloseq I am using:
# https://github.com/joey711/phyloseq/issues/223
fake.tax=as.matrix(particles.rar.mod@tax_table)
fake.tax=cbind(fake.tax,fake.tax)
fake.tax = subset(fake.tax, select = c("Modules","modules"))
fake.tax
colnames(fake.tax)=c("Modules","modules")
View(fake.tax)
tax_table(particles.rar.mod) <- fake.tax
tax_table(particles.rar.mod)
particles.rar.mod.aggr=tax_glom(particles.rar.mod, taxrank = "Modules")
rank_names(particles.rar.mod)
Nmod=ntaxa(particles.rar.mod.aggr) # your modules
Nsamp=nsamples(particles.rar.mod.aggr)
# --- Compute the proportions in each replicate/time point
ESVs.modules.totals=rowSums(particles.rar.mod.aggr@otu_table)
particles.rar.mod.aggr.prop=particles.rar.mod.aggr
particles.rar.mod.aggr.prop@otu_table=particles.rar.mod.aggr@otu_table/ESVs.modules.totals
plot(rowSums(particles.rar.mod.aggr.prop@otu_table)) # double check, should sum up to one
# --- Create association vectors to determine the preferred stage
Nrep=3 # number replicates
Nstage=5 # Number artificial vectors we create to test associations
founder=1*Nrep  # Number of bins in each stage, 1/founder is the null probablity
if(substrate == "Carrageenan"){ # one sample missing, 24C
early=(5*Nrep)-1 # same for other stages
}else{
early=5*Nrep # same for other stages
}
mid=2*Nrep
late=4*Nrep
all=12*Nrep
# --- Now create the vectors,
# ..... first the null vectors
profiles=matrix(0,nrow=(Nmod+Nstage),ncol=Nsamp)
profiles[1,(1:founder)]=1/founder
profiles[2,(founder+1):(founder+early)]=1/early
profiles[3,(founder+early+1):(founder+early+mid)]=1/mid
profiles[4,(founder+early+mid+1):(founder+early+mid+late)]=1/late
profiles[5,]=1/Nsamp
profiles
